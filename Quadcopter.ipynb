{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "* Standardize state\n",
    "    * Use upper_bounds and lower_bounds where known ahead of time. (defined by environment, i.e. in sim)\n",
    "    * Where bounds in a state dimension are unknown, scale by mean and std of everything in memory.\n",
    "    * Consider scaling some dimensions if they are not normally distributed.\n",
    "* Standardize and scale rewards\n",
    "    * Try automatically detecting if scaling (e.g. logarathmic) results in a more normal distribution.\n",
    "* Modulate noise while training\n",
    "    * Try implementing this using hyperopt.\n",
    "        * After each episode report training score and ask hyperopt for new noise parameters.\n",
    "        \n",
    "* After each episode:\n",
    "    * Check if test score has improved\n",
    "    * Save model weights if best score\n",
    "    * If test_score > training_score, reduce epsilon\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from ddpg_agent.contrib.physics_sim import PhysicsSim\n",
    "from collections import namedtuple\n",
    "\n",
    "class Task():\n",
    "    \"\"\"Task (environment) that defines the goal and provides feedback to the agent.\"\"\"\n",
    "    def __init__(self, init_pose=None, init_velocities=None, \n",
    "        init_angle_velocities=None, runtime=10., target_pos=None, \n",
    "        vert_dist_thresh=1, horiz_dist_thresh=1,\n",
    "        target_steps_within_goal=1,):\n",
    "        \"\"\"Initialize a Task object.\n",
    "        Params\n",
    "        ======\n",
    "            init_pose: initial position of the quadcopter in (x,y,z) dimensions and the Euler angles\n",
    "            init_velocities: initial velocity of the quadcopter in (x,y,z) dimensions\n",
    "            init_angle_velocities: initial radians/second for each of the three Euler angles\n",
    "            runtime: time limit for each episode\n",
    "            target_pos: target/goal (x,y,z) position for the agent\n",
    "        \"\"\"\n",
    "        # Simulation\n",
    "        self.sim = PhysicsSim(init_pose, init_velocities, init_angle_velocities, runtime) \n",
    "        # TODO: Make action_repeat align with agent.action_repeat\n",
    "        self.action_repeat = 3\n",
    "\n",
    "        self.state_size = self.action_repeat * 6 + 6\n",
    "        self.observation_space = Space( list(list(self.sim.lower_bounds) + \\\n",
    "                                             [ -math.pi ]*3)*self.action_repeat + [float('-inf')]*6, \n",
    "                                       list(list(self.sim.upper_bounds) + \\\n",
    "                                            [ math.pi ]*3)*self.action_repeat + [float('inf')]*6 )\n",
    "        self.action_space = Space([0,0,0,0], [900,900,900,900])\n",
    "        self.action_size = 4\n",
    "\n",
    "        # Goal\n",
    "        self.target_pos = target_pos if target_pos is not None else np.array([0., 0., 10.])\n",
    "        self.target_steps_within_goal = target_steps_within_goal\n",
    "        self.steps_within_goal = 0\n",
    "        self.horiz_dist_thresh = horiz_dist_thresh\n",
    "        self.vert_dist_thresh = vert_dist_thresh\n",
    "        \n",
    "        # History\n",
    "        self.step_history = []\n",
    "        \n",
    "    def reached_goal(self):\n",
    "        horiz_distance_from_goal = np.sqrt((self.sim.pose[0]-self.target_pos[0])**2\n",
    "                                           +(self.sim.pose[1]-self.target_pos[1])**2)\n",
    "        vert_distance_from_goal = np.abs(self.sim.pose[2]-self.target_pos[2])\n",
    "        return horiz_distance_from_goal < self.horiz_dist_thresh and \\\n",
    "                vert_distance_from_goal <= self.vert_dist_thresh\n",
    "    \n",
    "    def get_full_state(self):\n",
    "        return namedtuple(\"QuadcopterState\",[\n",
    "                             'x', 'y', 'z', 'phi', 'theta', 'psi', \n",
    "                             'x_velocity', 'y_velocity', 'z_velocity', \n",
    "                             'phi_velocity', 'theta_velocity', 'psi_velocity',\n",
    "                             'x_linear_accel','y_linear_accel','z_linear_accel',\n",
    "                             'phi_angular_accel','theta_angular_accel','psi_angular_accel',\n",
    "                            ])( *self.sim.pose, *self.sim.v, *self.sim.angular_v, \n",
    "                                *self.sim.linear_accel, *self.sim.angular_accels )\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Uses current pose of sim to return reward.\"\"\"\n",
    "        #reward = 1.-.3*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "        reward = 0\n",
    "        # Reward for staying at target altitude\n",
    "#         target_alt=self.target_pos[2]\n",
    "#         reward = .1*(target_alt - np.abs(self.sim.pose[2] - target_alt))/target_alt\n",
    "#         distance_from_goal = np.sqrt((self.sim.pose[0]-self.target_pos[0])**2\n",
    "#                  +(self.sim.pose[1]-self.target_pos[1])**2\n",
    "#                  +(self.sim.pose[2]-self.target_pos[2])**2)\n",
    "#         reward += 1-distance_from_goal/10.\n",
    "        #Intermediate reward for flying at altitude\n",
    "#         if np.abs(self.sim.pose[2] - self.target_pos[2]) < 1:\n",
    "#             reward += 1\n",
    "        vert_dist = np.abs(self.target_pos[2]-self.sim.pose[2])\n",
    "        if vert_dist<10:\n",
    "            reward += 10-vert_dist\n",
    "            \n",
    "        # Punish for high angular velocity\n",
    "        reward -= (self.sim.angular_v[0]/30.)**2 + (self.sim.angular_v[1]/30.)**2\n",
    "            \n",
    "        # Punishment for crashing (altitude < 1 m)\n",
    "#         if self.sim.pose[2]<=0: reward -= 1000\n",
    "#         if self.sim.pose[2]<2: reward -= 1\n",
    "        # Reward for being within goal radius\n",
    "#         horiz_distance_from_goal = np.sqrt((self.sim.pose[0]-self.target_pos[0])**2\n",
    "#                                            +(self.sim.pose[1]-self.target_pos[1])**2)\n",
    "        # Reward for going up\n",
    "        if self.sim.v[2]>0:\n",
    "            reward += 1\n",
    "        # Penalty for falling\n",
    "        if self.sim.v[2]<0:\n",
    "            reward -= .01*(self.sim.v[2]**2)\n",
    "            \n",
    "#         if self.reached_goal(): \n",
    "#             self.steps_within_goal += 1\n",
    "#             reward += 1\n",
    "# #             if self.steps_within_goal / self.action_repeat >= self.target_steps_within_goal: \n",
    "# #                 reward += 1000\n",
    "#         else:\n",
    "#             self.steps_within_goal = 0\n",
    "        return reward\n",
    "\n",
    "    def step(self, rotor_speeds):\n",
    "        \"\"\"Uses action to obtain next state, reward, done.\"\"\"\n",
    "        def zero_center_rotation(pose):\n",
    "            pose[3]=pose[3]-math.pi\n",
    "            pose[4]=pose[4]-math.pi\n",
    "            pose[5]=pose[5]-math.pi\n",
    "#             def _fix(p):\n",
    "#                 if p>math.pi: p -= math.pi\n",
    "#                 return p - math.pi/2.\n",
    "#             for p in [3,4,5]:\n",
    "#                 pose[p]=_fix(pose[p])\n",
    "            \n",
    "            return pose\n",
    "        reward = 0\n",
    "        pose_all = []\n",
    "        for _ in range(self.action_repeat):\n",
    "            done = self.sim.next_timestep(rotor_speeds) # update the sim pose and velocities\n",
    "            reward += self.get_reward() \n",
    "#             pose_all.append(self.sim.pose)\n",
    "            pose_all.append(zero_center_rotation(self.sim.pose))\n",
    "#         reward = np.tanh(reward)\n",
    "        next_state = list(np.concatenate(pose_all))+list(self.sim.v)+list(self.sim.angular_v)\n",
    "#             import pdb; pdb.set_trace()\n",
    "        # Punish and end episode for crashing\n",
    "        if self.sim.pose[2]<=0: \n",
    "#             reward -= 100\n",
    "            done = True\n",
    "        \n",
    "#         # Punish for excessive rotor speeds\n",
    "#         reward -= np.sum(((rotor_speeds-self.action_space.low)/(self.action_space.high-self.action_space.low))**2)\n",
    "        \n",
    "        # end episode if at goal state\n",
    "#         if self.steps_within_goal / self.action_repeat >= self.target_steps_within_goal: \n",
    "        if self.sim.pose[2] > self.target_pos[2]:\n",
    "            reward += 1000\n",
    "            done = True\n",
    "        # Scale reward. \n",
    "        # TODO: How can the agent detect need for reward scaling automatically?\n",
    "#         reward = np.log1p(reward) if reward>0 else np.log1p(-reward)\n",
    "\n",
    "        return next_state, reward, done, None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the sim to start a new episode.\"\"\"\n",
    "        self.sim.reset()\n",
    "        state = list(np.concatenate([self.sim.pose] * self.action_repeat)) + \\\n",
    "                list(self.sim.v) + list(self.sim.angular_v)\n",
    "        self.steps_within_goal = 0\n",
    "        return state\n",
    "\n",
    "class Space():\n",
    "    def __init__(self, low, high):\n",
    "        low = np.array(low)\n",
    "        high = np.array(high)\n",
    "        assert low.shape == high.shape,\\\n",
    "            \"Expected bounds to be of same shape.\"\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.shape = low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent.agent import DDPG, Q_a_frames_spec\n",
    "from ddpg_agent.visualizations import plot_quadcopter_episode\n",
    "\n",
    "task = Task(init_pose=np.array([0., 0., 10, 0., 0., 0.]),\n",
    "            #init_pose=np.array([0., 0., 10, math.pi/2., math.pi/2., 0.]), \n",
    "            init_velocities=np.array([0., 0., 0.]), \n",
    "            init_angle_velocities=np.array([0., 0., 0.]), \n",
    "            runtime=10., \n",
    "            vert_dist_thresh=1, horiz_dist_thresh=1, \n",
    "            target_steps_within_goal=25,\n",
    "            target_pos=np.array([0., 0., 20.]),\n",
    "           )\n",
    "\n",
    "# q_a_frames_spec = Q_a_frames_spec(task, nx=16, ny=16, na=11, x_dim=3, y_dim=4, a_dim=0)\n",
    "\n",
    "agent = DDPG(task, ou_mu=0, ou_theta=.02, ou_sigma=.5, \n",
    "             discount_factor=.7, replay_buffer_size=100000, replay_batch_size=512,\n",
    "             tau_actor=.2, tau_critic=.4, \n",
    "#              relu_alpha_actor=.01, relu_alpha_critic=.01,\n",
    "#              lr_actor=.00001, lr_critic=.0001, #activation_fn_actor='tanh',\n",
    "#              l2_reg_actor=.01, l2_reg_critic=.01, \n",
    "#              bn_momentum_actor=0, bn_momentum_critic=.7, \n",
    "#              q_a_frames_spec=q_a_frames_spec, \n",
    "#              do_preprocessing=False,\n",
    "#              input_bn_momentum_actor=.7,\n",
    "#              input_bn_momentum_critic=.7,\n",
    "#              activity_l2_reg=50,\n",
    "#              output_action_regularizer=10,\n",
    "            )\n",
    "# agent.print_summary()\n",
    "\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "# [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_callback(episode_num):\n",
    "    last_training_episode = agent.history.training_episodes[-1]\n",
    "    last_test_episode = agent.history.test_episodes[-1]\n",
    "    num_steps = last_training_episode.last_step - last_training_episode.first_step+1\n",
    "    message = \"Episode %i - epsilon: %7.4g, memory: %i, step: %i, training score: %6.2f, test score: %6.2f\"\\\n",
    "                %(episode_num, \n",
    "                  last_training_episode.epsilon, \n",
    "                  len(agent.memory), \n",
    "                  num_steps, \n",
    "                  last_training_episode.score,\n",
    "                  last_test_episode.score\n",
    "                 )\n",
    "    print(message)\n",
    "    \n",
    "    if episode_num%10==0:\n",
    "        fig = plot_quadcopter_episode(last_training_episode)\n",
    "        display(fig)\n",
    "        \n",
    "agent.set_episode_callback(episode_callback)\n",
    "\n",
    "def max_training_score_callback(episode):\n",
    "    last_training_episode = agent.history.training_episodes[-1]\n",
    "    print(\"New best training score.\")\n",
    "    fig = plot_quadcopter_episode(last_training_episode)\n",
    "    display(fig)\n",
    "    \n",
    "agent.set_max_training_score_callback(max_training_score_callback)\n",
    "    \n",
    "def max_test_score_callback(episode):\n",
    "    last_test_episode = agent.history.test_episodes[-1]\n",
    "    print(\"New best test score.\")\n",
    "    fig = plot_quadcopter_episode(last_test_episode)\n",
    "    display(fig)\n",
    "        \n",
    "agent.set_max_test_score_callback(max_test_score_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=200, eps_decay=1, run_tests=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=.5, run_tests=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=.25, run_tests=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=200, eps_decay=1, run_tests=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=400, eps_decay=0, run_tests=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent.visualizations import visualize_quad_agent\n",
    "\n",
    "# q_a_frame = agent.get_q_a_frames(q_a_frames_spec)\n",
    "agent.reset_episode()\n",
    "visualize_quad_agent(agent, Q_a_frames_spec(task, nx=16, ny=16, na=11, x_dim=3, y_dim=4, a_dim=0))\n",
    "visualize_quad_agent(agent, Q_a_frames_spec(task, nx=16, ny=16, na=11, x_dim=1, y_dim=2, a_dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.sim.init_pose=np.array([0,0,10,math.pi/2.,math.pi/2.,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory.action_means)\n",
    "print(np.sqrt(agent.memory.action_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.state_means, np.sqrt(agent.memory.state_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.reward_mean, np.sqrt(agent.memory.reward_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([s.reward for s in agent.memory.sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([s.next_state[2] for s in agent.memory.sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std([s.action[1] for s in agent.memory.sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
