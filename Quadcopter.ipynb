{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "* Standardize state\n",
    "    * Use upper_bounds and lower_bounds where known ahead of time. (defined by environment, i.e. in sim)\n",
    "    * Where bounds in a state dimension are unknown, scale by mean and std of everything in memory.\n",
    "    * Consider scaling some dimensions if they are not normally distributed.\n",
    "* Standardize and scale rewards\n",
    "    * Try automatically detecting if scaling (e.g. logarathmic) results in a more normal distribution.\n",
    "* Modulate noise while training\n",
    "    * Try implementing this using hyperopt.\n",
    "        * After each episode report training score and ask hyperopt for new noise parameters.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from ddpg_agent.contrib.physics_sim import PhysicsSim\n",
    "\n",
    "class Task():\n",
    "    \"\"\"Task (environment) that defines the goal and provides feedback to the agent.\"\"\"\n",
    "    def __init__(self, init_pose=None, init_velocities=None, \n",
    "        init_angle_velocities=None, runtime=10., target_pos=None, \n",
    "        vert_dist_thresh=1, horiz_dist_thresh=1,\n",
    "        target_steps_within_goal=1,):\n",
    "        \"\"\"Initialize a Task object.\n",
    "        Params\n",
    "        ======\n",
    "            init_pose: initial position of the quadcopter in (x,y,z) dimensions and the Euler angles\n",
    "            init_velocities: initial velocity of the quadcopter in (x,y,z) dimensions\n",
    "            init_angle_velocities: initial radians/second for each of the three Euler angles\n",
    "            runtime: time limit for each episode\n",
    "            target_pos: target/goal (x,y,z) position for the agent\n",
    "        \"\"\"\n",
    "        # Simulation\n",
    "        self.sim = PhysicsSim(init_pose, init_velocities, init_angle_velocities, runtime) \n",
    "        # TODO: Make action_repeat align with agent.action_repeat\n",
    "        self.action_repeat = 3\n",
    "\n",
    "        self.state_size = self.action_repeat * 6 + 6\n",
    "        self.observation_space = Space( list(list(self.sim.lower_bounds) + [ -1 ]*3)*self.action_repeat + [-1]*6, \n",
    "                                       list(list(self.sim.upper_bounds) + [ 1 ]*3)*self.action_repeat + [1]*6 )\n",
    "#         self.state_size = 6\n",
    "        self.action_space = Space([0,0,0,0], [900,900,900,900])\n",
    "        self.action_size = 4\n",
    "\n",
    "        # Goal\n",
    "        self.target_pos = target_pos if target_pos is not None else np.array([0., 0., 10.])\n",
    "        self.target_steps_within_goal = target_steps_within_goal\n",
    "        self.steps_within_goal = 0\n",
    "        self.horiz_dist_thresh = horiz_dist_thresh\n",
    "        self.vert_dist_thresh = vert_dist_thresh\n",
    "        \n",
    "    def reached_goal(self):\n",
    "        horiz_distance_from_goal = np.sqrt((self.sim.pose[0]-self.target_pos[0])**2\n",
    "                                           +(self.sim.pose[1]-self.target_pos[1])**2)\n",
    "        vert_distance_from_goal = np.abs(self.sim.pose[2]-self.target_pos[2])\n",
    "        return horiz_distance_from_goal < self.horiz_dist_thresh and \\\n",
    "                vert_distance_from_goal <= self.vert_dist_thresh\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Uses current pose of sim to return reward.\"\"\"\n",
    "        #reward = 1.-.3*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "        reward = 0\n",
    "        # Reward for staying at target altitude\n",
    "        target_alt=self.target_pos[2]\n",
    "        reward = .1*(target_alt - np.abs(self.sim.pose[2] - target_alt))/target_alt\n",
    "        #Intermediate reward for flying at altitude\n",
    "#         if np.abs(self.sim.pose[2] - self.target_pos[2]) < 1:\n",
    "#             reward += 1\n",
    "        # Punishment for crashing (altitude < 1 m)\n",
    "#         if self.sim.pose[2]<=0: reward -= 1000\n",
    "#         if self.sim.pose[2]<2: reward -= 1\n",
    "        # Reward for being within goal radius\n",
    "#         horiz_distance_from_goal = np.sqrt((self.sim.pose[0]-self.target_pos[0])**2\n",
    "#                                            +(self.sim.pose[1]-self.target_pos[1])**2)\n",
    "        # Reward for going up\n",
    "#         if self.sim.v[2]>0:\n",
    "#             reward += .001\n",
    "            \n",
    "        if self.reached_goal(): \n",
    "            self.steps_within_goal += 1\n",
    "            reward += 1\n",
    "#             if self.steps_within_goal / self.action_repeat >= self.target_steps_within_goal: \n",
    "#                 reward += 1000\n",
    "        else:\n",
    "            self.steps_within_goal = 0\n",
    "        return reward\n",
    "\n",
    "    def step(self, rotor_speeds):\n",
    "        \"\"\"Uses action to obtain next state, reward, done.\"\"\"\n",
    "        reward = 0\n",
    "        pose_all = []\n",
    "        for _ in range(self.action_repeat):\n",
    "            done = self.sim.next_timestep(rotor_speeds) # update the sim pose and velocities\n",
    "            reward += self.get_reward() \n",
    "            pose_all.append(self.sim.pose)\n",
    "        next_state = list(np.concatenate(pose_all))+list(self.sim.v)+list(self.sim.angular_v)\n",
    "#             import pdb; pdb.set_trace()\n",
    "        # Punish and end episode for crashing\n",
    "        if self.sim.pose[2]<=0: \n",
    "            reward -= 100\n",
    "            done = True\n",
    "        # end episode if at goal state\n",
    "        if self.steps_within_goal / self.action_repeat >= self.target_steps_within_goal: \n",
    "            reward += 100\n",
    "            done = True\n",
    "        # Scale reward. \n",
    "        # TODO: How can the agent detect need for reward scaling automatically?\n",
    "        reward = np.log1p(reward) if reward>0 else np.log1p(-reward)\n",
    "        return next_state, reward, done, None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the sim to start a new episode.\"\"\"\n",
    "        self.sim.reset()\n",
    "        state = list(np.concatenate([self.sim.pose] * self.action_repeat)) + \\\n",
    "                list(self.sim.v) + list(self.sim.angular_v)\n",
    "#         state = self.sim.pose\n",
    "        self.steps_within_goal = 0\n",
    "        return state\n",
    "\n",
    "class Space():\n",
    "    def __init__(self, low, high):\n",
    "        low = np.array(low)\n",
    "        high = np.array(high)\n",
    "        assert low.shape == high.shape,\\\n",
    "            \"Expected bounds to be of same shape.\"\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.shape = low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme=max(np.abs(min(episode.rewards)),np.abs(max(episode.rewards)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_episode(episode):\n",
    "    # goal_position = agent.env.target_pos\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    fig.suptitle(\"Episode %i, score: %.3f, epsilon: %.4g\"%(episode.episode_idx, episode.score, episode.epsilon))\n",
    "\n",
    "    main_cols = gridspec.GridSpec(1, 3, figure=fig)\n",
    "    right_col_grid = main_cols[1:].subgridspec(2,3,wspace=.2,hspace=.3)\n",
    "\n",
    "    min_reward=min(episode.rewards)\n",
    "    max_reward=max(episode.rewards)\n",
    "    if min_reward==max_reward:\n",
    "        min_reward=-1\n",
    "        max_reward=1\n",
    "    extreme=max(np.abs(min_reward),np.abs(max_reward))\n",
    "    reward_norm = mpl.colors.SymLogNorm(linthresh=1, linscale=3, vmin=-extreme, vmax=extreme)\n",
    "    reward_cmap = mpl.cm.ScalarMappable(norm=reward_norm, cmap=mpl.cm.get_cmap('RdYlGn'))\n",
    "    reward_cmap.set_array([])\n",
    "#     reward_cmap.set_clim(min(episode.rewards), max(episode.rewards))\n",
    "    \n",
    "    pos_ax = fig.add_subplot(main_cols[0], projection='3d', title=\"Flight Path\")\n",
    "    pos_scatter = pos_ax.scatter([s[0] for s in episode.states], [s[1] for s in episode.states], \n",
    "                                 [s[2] for s in episode.states], \n",
    "                                 c=[reward_cmap.to_rgba(r) for r in episode.rewards],\n",
    "                                 edgecolor='k', )\n",
    "\n",
    "    fig.colorbar(reward_cmap, ax=pos_ax, shrink=.8, pad=.02, label=\"reward\", orientation='horizontal')\n",
    "\n",
    "    alt_ax = fig.add_subplot(right_col_grid[0,0], title=\"Altitude\", xlabel='step')\n",
    "    alt_ax.plot([s[2] for s in episode.states], color='magenta')\n",
    "\n",
    "\n",
    "    actions_grid = right_col_grid[0,2].subgridspec(4,1)\n",
    "    def plot_action(i):\n",
    "#         a_colors=['darkorange','darkgoldenrod','peru','lightsalmon']\n",
    "        ax = fig.add_subplot(actions_grid[i], ylim=(-100,1000), xlabel='step', yticks=[0,400,800])\n",
    "#         if i==0: ax.set_title('Actions')\n",
    "        ax.plot([a[i] for a in episode.raw_actions], color='gray', label='raw action')\n",
    "        ax.plot([a[i] for a in episode.actions], label='action + noise', color='darkorange')#a_colors[i])\n",
    "        if i==0: ax.legend(loc='lower center', bbox_to_anchor=(.5,.9))\n",
    "    for i in range(4): plot_action(i)\n",
    "    \n",
    "    def plot_state(ax,i):\n",
    "        s_colors=['slateblue','royalblue','magenta','steelblue','skyblue','deepskyblue']\n",
    "        s_labels=['x pos','y pos','altitude','roll', 'pitch', 'yaw']\n",
    "        state=np.array([s[i] for s in episode.states])\n",
    "        ax.plot(normalize(state.reshape(-1,1),axis=0), color=s_colors[i], label=s_labels[i])\n",
    "\n",
    "    rot_ax = fig.add_subplot(right_col_grid[1,0], title=\"Orientation\", xlabel='step')\n",
    "    for i in range(3,6): plot_state(rot_ax,i)\n",
    "    rot_ax.legend(loc='upper right')\n",
    "    \n",
    "    v_ax = fig.add_subplot(right_col_grid[1,1], title=\"Velocity\", xlabel='step')\n",
    "    #import pdb; pdb.set_trace()\n",
    "    v_ax.plot([s[18] for s in episode.states], label=\"x\")\n",
    "    v_ax.plot([s[19] for s in episode.states], label=\"y\")\n",
    "    v_ax.plot([s[20] for s in episode.states], label=\"z\")\n",
    "    v_ax.legend(loc='upper right')\n",
    "    \n",
    "    \n",
    "    ang_v_ax = fig.add_subplot(right_col_grid[1,2], title=\"Angular Velocity\", xlabel='step')\n",
    "    ang_v_ax.plot([s[21] for s in episode.states], label=\"x\")\n",
    "    ang_v_ax.plot([s[22] for s in episode.states], label=\"y\")\n",
    "    ang_v_ax.plot([s[23] for s in episode.states], label=\"z\")\n",
    "    ang_v_ax.legend(loc='upper right')\n",
    "\n",
    "    horiz_pos_ax = fig.add_subplot(right_col_grid[0,1], title=\"Horizontal Position\")\n",
    "    horiz_pos_ax.scatter([s[0] for s in episode.states], [s[1] for s in episode.states], \n",
    "                      c=[reward_cmap.to_rgba(r) for r in episode.rewards],\n",
    "                      edgecolor='k',)\n",
    "        \n",
    "    fig.show()\n",
    "# for ep in agent.history.training_episodes[:6]:\n",
    "#     plot_episode(ep)\n",
    "# plot_episode(agent.history.test_episodes[71])\n",
    "# plot_episode(agent.history.training_episodes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "from ddpg_agent.agent import DDPG, Q_a_frames_spec\n",
    "\n",
    "task = Task(init_pose=np.array([0., 0., 8.5, 0., 0., 0.]), \n",
    "            init_velocities=np.array([0., 0., 0.]), \n",
    "            init_angle_velocities=np.array([0., 0., 0.]), \n",
    "            runtime=10., \n",
    "            vert_dist_thresh=1, horiz_dist_thresh=1, \n",
    "            target_steps_within_goal=25,\n",
    "            target_pos=np.array([0., 0., 10.]),\n",
    "           )\n",
    "\n",
    "q_a_frames_spec = Q_a_frames_spec(task, nx=16, ny=16, na=11, x_dim=4, y_dim=2, a_dim=0)\n",
    "\n",
    "agent = DDPG(task, ou_mu=0, ou_theta=.1, ou_sigma=.25, \n",
    "             discount_factor=.999, replay_buffer_size=100000, replay_batch_size=1024,\n",
    "             tau_actor=.1, tau_critic=.2, \n",
    "             relu_alpha_actor=.01, relu_alpha_critic=.01,\n",
    "             lr_actor=.0001, lr_critic=.0005, activation_fn_actor='tanh',\n",
    "#              l2_reg_actor=.01, l2_reg_critic=.01, \n",
    "             bn_momentum_actor=0, bn_momentum_critic=.7, \n",
    "             q_a_frames_spec=q_a_frames_spec, do_preprocessing=False,\n",
    "             input_bn_momentum_actor=.7,\n",
    "             input_bn_momentum_critic=.7,\n",
    "#              activity_l2_reg=.01,\n",
    "             output_action_regularizer=10,\n",
    "            )\n",
    "# agent.print_summary()\n",
    "\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "# [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - epsilon: 1980, memory size: 27, num steps: 27, training score: -95.15, test score: -95.90\n",
      "Episode 2 - epsilon: 1960, memory size: 49, num steps: 22, training score: -96.25, test score: -95.90\n",
      "Episode 3 - epsilon: 1940, memory size: 71, num steps: 22, training score: -96.48, test score: -95.90\n",
      "Episode 4 - epsilon: 1920, memory size: 95, num steps: 24, training score: -96.10, test score: -95.90\n",
      "Episode 5 - epsilon: 1900, memory size: 120, num steps: 25, training score: -95.88, test score: -95.90\n",
      "Episode 6 - epsilon: 1880, memory size: 144, num steps: 24, training score: -96.16, test score: -95.90\n",
      "Episode 7 - epsilon: 1860, memory size: 167, num steps: 23, training score: -96.12, test score: -95.90\n",
      "Episode 8 - epsilon: 1840, memory size: 190, num steps: 23, training score: -96.18, test score: -95.90\n",
      "Episode 9 - epsilon: 1820, memory size: 211, num steps: 21, training score: -96.60, test score: -95.90\n",
      "Episode 10 - epsilon: 1800, memory size: 235, num steps: 24, training score: -95.94, test score: -95.90\n",
      "Episode 11 - epsilon: 1780, memory size: 258, num steps: 23, training score: -96.19, test score: -95.90\n",
      "Episode 12 - epsilon: 1760, memory size: 285, num steps: 27, training score: -95.86, test score: -95.90\n",
      "Episode 13 - epsilon: 1740, memory size: 309, num steps: 24, training score: -96.04, test score: -95.90\n",
      "Episode 14 - epsilon: 1720, memory size: 332, num steps: 23, training score: -96.24, test score: -95.90\n",
      "Episode 15 - epsilon: 1700, memory size: 355, num steps: 23, training score: -96.00, test score: -95.90\n",
      "Episode 16 - epsilon: 1680, memory size: 382, num steps: 27, training score: -95.56, test score: -95.90\n",
      "Episode 17 - epsilon: 1660, memory size: 407, num steps: 25, training score: -95.79, test score: -95.90\n",
      "Episode 18 - epsilon: 1640, memory size: 429, num steps: 22, training score: -96.37, test score: -95.90\n",
      "Episode 19 - epsilon: 1620, memory size: 460, num steps: 31, training score: -94.35, test score: -95.90\n",
      "Episode 20 - epsilon: 1600, memory size: 482, num steps: 22, training score: -96.34, test score: -95.90\n",
      "Episode 21 - epsilon: 1580, memory size: 510, num steps: 28, training score: -94.99, test score: -95.90\n",
      "Episode 22 - epsilon: 1560, memory size: 531, num steps: 21, training score: -96.62, test score: -95.90\n",
      "Episode 23 - epsilon: 1540, memory size: 553, num steps: 22, training score: -96.48, test score: -95.90\n",
      "Episode 24 - epsilon: 1520, memory size: 576, num steps: 23, training score: -96.18, test score: -95.90\n",
      "Episode 25 - epsilon: 1500, memory size: 600, num steps: 24, training score: -95.92, test score: -95.90\n",
      "Episode 26 - epsilon: 1480, memory size: 623, num steps: 23, training score: -96.18, test score: -95.90\n",
      "Episode 27 - epsilon: 1460, memory size: 643, num steps: 20, training score: -96.61, test score: -95.90\n",
      "Episode 28 - epsilon: 1440, memory size: 666, num steps: 23, training score: -96.16, test score: -95.90\n",
      "Episode 29 - epsilon: 1420, memory size: 688, num steps: 22, training score: -96.43, test score: -95.90\n",
      "Episode 30 - epsilon: 1400, memory size: 709, num steps: 21, training score: -96.61, test score: -95.90\n",
      "Episode 31 - epsilon: 1380, memory size: 732, num steps: 23, training score: -96.27, test score: -95.90\n",
      "Episode 32 - epsilon: 1360, memory size: 752, num steps: 20, training score: -96.65, test score: -95.90\n",
      "Episode 33 - epsilon: 1340, memory size: 777, num steps: 25, training score: -95.62, test score: -95.90\n",
      "Episode 34 - epsilon: 1320, memory size: 803, num steps: 26, training score: -95.41, test score: -95.90\n",
      "Episode 35 - epsilon: 1300, memory size: 828, num steps: 25, training score: -95.92, test score: -95.90\n",
      "Episode 36 - epsilon: 1280, memory size: 856, num steps: 28, training score: -95.45, test score: -95.90\n",
      "Episode 37 - epsilon: 1260, memory size: 878, num steps: 22, training score: -96.43, test score: -95.90\n",
      "Episode 38 - epsilon: 1240, memory size: 901, num steps: 23, training score: -96.19, test score: -95.90\n",
      "Episode 39 - epsilon: 1220, memory size: 923, num steps: 22, training score: -96.30, test score: -95.90\n",
      "Episode 40 - epsilon: 1200, memory size: 943, num steps: 20, training score: -96.67, test score: -95.90\n",
      "Episode 41 - epsilon: 1180, memory size: 966, num steps: 23, training score: -96.10, test score: -95.90\n",
      "Episode 42 - epsilon: 1160, memory size: 987, num steps: 21, training score: -96.51, test score: -95.90\n",
      "Episode 43 - epsilon: 1140, memory size: 1010, num steps: 23, training score: -96.01, test score: -95.90\n",
      "Episode 44 - epsilon: 1120, memory size: 1038, num steps: 28, training score: -95.02, test score: -94.83\n",
      "Episode 45 - epsilon: 1100, memory size: 1060, num steps: 22, training score: -96.43, test score: -95.47\n",
      "Episode 46 - epsilon: 1080, memory size: 1086, num steps: 26, training score: -95.48, test score: -95.22\n",
      "Episode 47 - epsilon: 1060, memory size: 1110, num steps: 24, training score: -95.97, test score: -95.80\n",
      "Episode 48 - epsilon: 1040, memory size: 1134, num steps: 24, training score: -95.97, test score: -94.73\n",
      "Episode 49 - epsilon: 1020, memory size: 1158, num steps: 24, training score: -96.00, test score: -96.09\n",
      "Episode 50 - epsilon: 1000, memory size: 1177, num steps: 19, training score: -96.84, test score: -96.41\n",
      "Episode 51 - epsilon: 980, memory size: 1198, num steps: 21, training score: -96.54, test score: -92.13\n",
      "Episode 52 - epsilon: 960, memory size: 1224, num steps: 26, training score: -95.44, test score: -93.72\n",
      "Episode 53 - epsilon: 940, memory size: 1249, num steps: 25, training score: -95.76, test score: -93.86\n",
      "Episode 54 - epsilon: 920, memory size: 1267, num steps: 18, training score: -97.10, test score: -94.43\n",
      "Episode 55 - epsilon: 900, memory size: 1291, num steps: 24, training score: -96.06, test score: -94.26\n",
      "Episode 56 - epsilon: 880, memory size: 1315, num steps: 24, training score: -95.92, test score: -94.18\n",
      "Episode 57 - epsilon: 860, memory size: 1337, num steps: 22, training score: -96.36, test score: -95.07\n",
      "Episode 58 - epsilon: 840, memory size: 1358, num steps: 21, training score: -96.55, test score: -95.25\n",
      "Episode 59 - epsilon: 820, memory size: 1382, num steps: 24, training score: -95.93, test score: -93.42\n",
      "Episode 60 - epsilon: 800, memory size: 1411, num steps: 29, training score: -95.09, test score: -93.51\n",
      "Episode 61 - epsilon: 780, memory size: 1433, num steps: 22, training score: -96.20, test score: -94.42\n",
      "Episode 62 - epsilon: 760, memory size: 1456, num steps: 23, training score: -96.06, test score: -94.30\n",
      "Episode 63 - epsilon: 740, memory size: 1479, num steps: 23, training score: -96.23, test score: -94.44\n",
      "Episode 64 - epsilon: 720, memory size: 1501, num steps: 22, training score: -96.38, test score: -93.93\n",
      "Episode 65 - epsilon: 700, memory size: 1523, num steps: 22, training score: -96.44, test score: -93.99\n",
      "Episode 66 - epsilon: 680, memory size: 1548, num steps: 25, training score: -95.79, test score: -94.18\n",
      "Episode 67 - epsilon: 660, memory size: 1570, num steps: 22, training score: -96.44, test score: -94.27\n",
      "Episode 68 - epsilon: 640, memory size: 1594, num steps: 24, training score: -96.05, test score: -94.38\n",
      "Episode 69 - epsilon: 620, memory size: 1618, num steps: 24, training score: -96.29, test score: -94.98\n",
      "Episode 70 - epsilon: 600, memory size: 1641, num steps: 23, training score: -96.12, test score: -94.89\n",
      "Episode 71 - epsilon: 580, memory size: 1664, num steps: 23, training score: -96.00, test score: -95.28\n",
      "Episode 72 - epsilon: 560, memory size: 1684, num steps: 20, training score: -96.78, test score: -95.09\n",
      "Episode 73 - epsilon: 540, memory size: 1705, num steps: 21, training score: -96.46, test score: -94.88\n",
      "Episode 74 - epsilon: 520, memory size: 1727, num steps: 22, training score: -96.10, test score: -94.91\n",
      "Episode 75 - epsilon: 500, memory size: 1752, num steps: 25, training score: -95.57, test score: -95.47\n",
      "Episode 76 - epsilon: 480, memory size: 1774, num steps: 22, training score: -96.21, test score: -94.86\n",
      "Episode 77 - epsilon: 460, memory size: 1798, num steps: 24, training score: -96.02, test score: -94.81\n",
      "Episode 78 - epsilon: 440, memory size: 1820, num steps: 22, training score: -96.38, test score: -94.87\n",
      "Episode 79 - epsilon: 420, memory size: 1841, num steps: 21, training score: -96.59, test score: -94.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80 - epsilon: 400, memory size: 1865, num steps: 24, training score: -95.84, test score: -95.10\n",
      "Episode 81 - epsilon: 380, memory size: 1890, num steps: 25, training score: -95.75, test score: -94.97\n",
      "Episode 82 - epsilon: 360, memory size: 1913, num steps: 23, training score: -95.92, test score: -94.92\n",
      "Episode 83 - epsilon: 340, memory size: 1938, num steps: 25, training score: -95.89, test score: -95.03\n",
      "Episode 84 - epsilon: 320, memory size: 1963, num steps: 25, training score: -95.70, test score: -95.56\n",
      "Episode 85 - epsilon: 300, memory size: 1986, num steps: 23, training score: -96.12, test score: -95.21\n",
      "Episode 86 - epsilon: 280, memory size: 2014, num steps: 28, training score: -94.82, test score: -95.50\n",
      "Episode 87 - epsilon: 260, memory size: 2035, num steps: 21, training score: -96.52, test score: -95.70\n",
      "Episode 88 - epsilon: 240, memory size: 2058, num steps: 23, training score: -96.00, test score: -95.78\n",
      "Episode 89 - epsilon: 220, memory size: 2085, num steps: 27, training score: -95.13, test score: -95.80\n",
      "Episode 90 - epsilon: 200, memory size: 2110, num steps: 25, training score: -95.65, test score: -95.90\n",
      "Episode 91 - epsilon: 180, memory size: 2135, num steps: 25, training score: -95.66, test score: -95.95\n",
      "Episode 92 - epsilon: 160, memory size: 2160, num steps: 25, training score: -95.69, test score: -95.97\n",
      "Episode 93 - epsilon: 140, memory size: 2183, num steps: 23, training score: -95.91, test score: -96.07\n",
      "Episode 94 - epsilon: 120, memory size: 2204, num steps: 21, training score: -96.42, test score: -96.00\n",
      "Episode 95 - epsilon: 100, memory size: 2229, num steps: 25, training score: -95.63, test score: -96.06\n",
      "Episode 96 - epsilon: 80, memory size: 2251, num steps: 22, training score: -96.34, test score: -95.97\n",
      "Episode 97 - epsilon: 60, memory size: 2281, num steps: 30, training score: -94.54, test score: -96.00\n",
      "Episode 98 - epsilon: 40, memory size: 2306, num steps: 25, training score: -95.69, test score: -95.86\n",
      "Episode 99 - epsilon: 20, memory size: 2331, num steps: 25, training score: -95.91, test score: -95.79\n",
      "Episode 100 - epsilon: 0, memory size: 2356, num steps: 25, training score: -95.78, test score: -95.83\n"
     ]
    }
   ],
   "source": [
    "agent.train_n_episodes(100, eps=2000, eps_decay=20, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_n_episodes(100, eps=100, eps_decay=1, action_repeat=1, run_tests=True, gen_q_a_frames_every_n_steps=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.env.target_pos=np.array([0,0,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.memory.memory.clear()\n",
    "# agent.env.target_steps_within_goal=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ep in agent.history.training_episodes[-10:]:\n",
    "    plot_episode(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode(agent.history.training_episodes[403])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode(agent.history.test_episodes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# with tf.keras.backend.get_session() as sess:\n",
    "#     tf.global_variables_initializer().run(session=sess)\n",
    "#     summary, _ = sess.run(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.actor_local.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# bn=[l for l in agent.actor_local.model.layers if type(l) is keras.layers.normalization.BatchNormalization][0]\n",
    "# bn.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([e.score for e in agent.history.training_episodes ]), \\\n",
    "    np.mean([e.score for e in agent.history.test_episodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(agent.history.test_episodes[-1].actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
